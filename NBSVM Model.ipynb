{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package import\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "import os, math\n",
    "import re, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from stop_words import get_stop_words\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data \n",
    "def load_data(data_file):\n",
    "    return pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training set, testing set, and result submission set \n",
    "train = load_data(\"train.csv\")\n",
    "test = load_data(\"test.csv\")\n",
    "result = load_data(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 6 labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline_model:\n",
    "    def __init__(self, train, test, result):\n",
    "    \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.result = result \n",
    "\n",
    "        \n",
    "    def pre_processing(self):\n",
    "        \n",
    "        # data preprocessing: create a none column if a text comment is not labels with toxicity \n",
    "        self.train['none'] = 1-self.train[labels].max(axis = 1)\n",
    "        # data precessing: fill in \"unknown\" for rows with missing comments \n",
    "        self.train['comment_text'].fillna(\"unknown\", inplace = True)\n",
    "        self.test['comment_text'].fillna(\"unknown\", inplace = True)\n",
    " \n",
    "        # return processed training set, and testing set\n",
    "        return self.train, self.test\n",
    "\n",
    "\n",
    "    def build_feature(self):\n",
    "        \n",
    "        self.train, self.test = self.pre_processing()\n",
    "        \n",
    "        # create a set of punctuations, numbers, and special characters\n",
    "        re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])1234567890')\n",
    "        \n",
    "        # create bag of words representation\n",
    "        def tokenize(s): \n",
    "            return re_tok.sub(r' \\1 ', s).split()\n",
    "            \n",
    "        n = train.shape[0]\n",
    "        \n",
    "        # use sklearn tfidf to compute weights and implement feature generation   \n",
    "        vec = TfidfVectorizer(ngram_range = (1,2), \n",
    "                              tokenizer = tokenize,\n",
    "                              min_df = 3, \n",
    "                              max_df = 0.9, \n",
    "                              strip_accents = 'unicode', \n",
    "                              use_idf = 1,\n",
    "                              smooth_idf = 1, \n",
    "                              sublinear_tf = 1 )\n",
    "        \n",
    "        # create two sparse matrix where contain small number of non-zero elements\n",
    "        self.train_doc = vec.fit_transform(self.train['comment_text'])\n",
    "        self.test_doc = vec.transform(self.test['comment_text'])\n",
    "        \n",
    "        return self.train_doc, self.test_doc\n",
    "    \n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.train_doc, self.test_doc = self.build_feature()\n",
    "        \n",
    "        # smoothing with naive base\n",
    "        def ratio(y_i, y):\n",
    "            p = self.train_doc[y == y_i].sum(0)\n",
    "            return (p + 1)/((y == y_i).sum() + 1)\n",
    "        \n",
    "        # fit the model with logistic regression \n",
    "        def get_model(y):\n",
    "            y = y.values\n",
    "            r = np.log(ratio(1,y) / ratio(0,y))\n",
    "            model = LogisticRegression(C = 4, \n",
    "                                       dual = True)\n",
    "            x_nb = self.train_doc.multiply(r)\n",
    "            return model.fit(x_nb, y), r\n",
    "\n",
    "        self.preds = np.zeros((len(self.test), len(labels)))\n",
    "        \n",
    "        # fit one label a time \n",
    "        for i, j in enumerate(labels):\n",
    "            print('fit', j)\n",
    "        \n",
    "            model,r = get_model(self.train[j])\n",
    "            self.preds[:,i] = model.predict_proba(self.test_doc.multiply(r))[:,1]\n",
    "            \n",
    "        return self.preds\n",
    "    \n",
    "    \n",
    "    def output(self):\n",
    "        \n",
    "        self.preds = self.build_model()\n",
    "        \n",
    "        # create the submission file \n",
    "        submission = pd.DataFrame({'id': self.result[\"id\"]})\n",
    "        result_submission = pd.concat([submission, pd.DataFrame(self.preds, \n",
    "                                                                columns = labels)], axis=1)\n",
    "        result_submission.to_csv('NBSVM_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline_model(train, test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baochen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baochen/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "baseline.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
